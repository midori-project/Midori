/**
 * üéØ LLM Adapter - Main orchestrator LLM interface
 * ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ multiple providers ‡πÅ‡∏•‡∏∞ fallback logic
 * Server-side version (‡πÉ‡∏ä‡πâ fs/promises ‡πÑ‡∏î‡πâ)
 */

import fs from 'fs/promises';
import path from 'path';
import * as yaml from 'js-yaml';
import { OpenAIProvider } from './openaiProvider';
import { LLMConfig, LLMProvider, LLMRequest, LLMResponse, TokenUsage } from './types';

interface AgentConfig {
  model: LLMConfig;
}

export class LLMAdapter {
  private providers: Map<string, LLMProvider> = new Map();
  private config: LLMConfig | null = null;
  private systemPrompts: Map<string, string> = new Map();
  private unreliableModels: Set<string> = new Set(); // ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° models ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤

  constructor() {
    // ‡πÑ‡∏°‡πà auto-initialize ‡πÉ‡∏ô constructor - ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å init methods ‡πÅ‡∏¢‡∏Å
  }

  async initialize(): Promise<void> {
    await this.initializeProviders();
    await this.loadConfig(); 
    await this.loadSystemPrompts();
  }

  private async initializeProviders(): Promise<void> {
    try {
      // Initialize OpenAI provider
      const apiKey = process.env.OPENAI_API_KEY;
      if (apiKey) {
        const openaiProvider = new OpenAIProvider(apiKey);
        this.providers.set('openai', openaiProvider);
        console.log('‚úÖ OpenAI provider initialized');
      } else {
        console.warn('‚ö†Ô∏è OpenAI API key not found');
      }
    } catch (error) {
      console.warn('‚ö†Ô∏è Failed to initialize LLM providers:', error);
    }
  }

  async loadConfig(): Promise<void> {
    try {
      // Use absolute path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ path ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ô‡∏à‡∏≤‡∏Å directory ‡∏≠‡∏∑‡πà‡∏ô
      const projectRoot = process.env.MIDORI_PROJECT_ROOT || process.cwd();
      console.log('üîç Loading config from:', { projectRoot, cwd: process.cwd() });
      const configPath = path.join(projectRoot, 'src/midori/agents/orchestrator/agent.yaml');
      console.log('üìÑ Config path:', configPath);
      const configFile = await fs.readFile(configPath, 'utf-8');
      const agentConfig = yaml.load(configFile) as AgentConfig;
      
      this.config = agentConfig.model;
      console.log('‚öôÔ∏è LLM config loaded:', {
        model: this.config.name,
        temperature: this.config.temperature,
        fallback: this.config.fallback?.name
      });
    } catch (error) {
      console.error('‚ùå Failed to load LLM config:', error);
      throw error;
    }
  }

  async loadSystemPrompts(): Promise<void> {
    try {
      const promptsPath = path.join(process.cwd(), 'src/midori/agents/orchestrator/prompts');
      
      const prompts = [
        { key: 'system', file: 'system-prompt.md' },
        { key: 'tasks', file: 'task_templates.md' },
        { key: 'guardrails', file: 'guardrails.md' }
      ];

      for (const { key, file } of prompts) {
        try {
          const content = await fs.readFile(path.join(promptsPath, file), 'utf-8');
          this.systemPrompts.set(key, content);
        } catch (error) {
          console.warn(`‚ö†Ô∏è Failed to load ${file}:`, error);
        }
      }

      console.log('‚úÖ System prompts loaded:', Array.from(this.systemPrompts.keys()));
    } catch (error) {
      console.error('‚ùå Failed to load system prompts:', error);
      throw error;
    }
  }

  async callLLM(
    prompt: string, 
    options: {
      useSystemPrompt?: boolean;
      model?: string;
      temperature?: number;
      maxTokens?: number;
    } = {}
  ): Promise<LLMResponse> {
    if (!this.config) {
      throw new Error('LLM config not loaded. Call initialize() first.');
    }

    const config = this.config;

    // Build system prompt
    let systemPrompt = '';
    if (options.useSystemPrompt !== false) {
      const systemContent = this.systemPrompts.get('system') || '';
      const tasksContent = this.systemPrompts.get('tasks') || '';
      const guardrailsContent = this.systemPrompts.get('guardrails') || '';
      systemPrompt = `${systemContent}\n\n${tasksContent}\n\n${guardrailsContent}`;
    }

    const request: LLMRequest = {
      prompt,
      systemPrompt: systemPrompt || undefined,
      model: options.model || config.name,
      temperature: options.temperature ?? config.temperature,
      maxTokens: options.maxTokens || config.max_completion_tokens || config.max_tokens
    };

    // Check if model is blacklisted
    const modelName = request.model || 'gpt-4o-mini';
    if (this.unreliableModels.has(modelName)) {
      console.warn(`‚ö†Ô∏è Model ${modelName} is blacklisted, skipping to fallback`);
    } else {
      // Try primary provider
      const primaryProvider = this.getProvider(modelName);
      if (primaryProvider && await primaryProvider.isAvailable()) {
        try {
          console.log(`üöÄ Calling ${modelName}...`);
          const response = await primaryProvider.call(request);
          
          // Check for empty or invalid response
          if (!response?.content || response.content.trim() === '') {
            console.warn(`‚ö†Ô∏è Empty response from ${modelName}, marking as unreliable`);
            this.markModelAsUnreliable(modelName);
          } else {
            console.log(`‚úÖ ${modelName} responded successfully`);
            return response;
          }
        } catch (error) {
          console.warn(`‚ö†Ô∏è Primary provider failed:`, error);
          this.markModelAsUnreliable(modelName);
          // Continue to fallback logic instead of re-throwing
        }
      }
    }

    // Try fallback provider
    if (config.fallback) {
      const fallbackModelName = config.fallback.name;
      
      // Check if fallback model is also blacklisted
      if (this.unreliableModels.has(fallbackModelName)) {
        console.warn(`‚ö†Ô∏è Fallback model ${fallbackModelName} is also blacklisted, trying alternative`);
        
        // Try gpt-4o-mini as last resort if it's not the current fallback
        if (fallbackModelName !== 'gpt-4o-mini' && !this.unreliableModels.has('gpt-4o-mini')) {
          const alternativeProvider = this.getProvider('gpt-4o-mini');
          if (alternativeProvider && await alternativeProvider.isAvailable()) {
            console.log(`üîÑ Using alternative model: gpt-4o-mini`);
            const alternativeRequest = {
              ...request,
              model: 'gpt-4o-mini',
              // ‡πÉ‡∏ä‡πâ temperature ‡∏à‡∏≤‡∏Å fallback config ‡∏´‡∏£‡∏∑‡∏≠ default
              temperature: config.fallback?.temperature
            };
            return await alternativeProvider.call(alternativeRequest);
          }
        }
      } else {
        const fallbackProvider = this.getProvider(fallbackModelName);
        if (fallbackProvider && await fallbackProvider.isAvailable()) {
          console.log(`üîÑ Falling back to ${fallbackModelName}...`);
          const fallbackRequest = {
            ...request,
            model: fallbackModelName,
            temperature: config.fallback.temperature
          };
          try {
            const response = await fallbackProvider.call(fallbackRequest);
            
            // Check fallback response quality too
            if (!response?.content || response.content.trim() === '') {
              console.warn(`‚ö†Ô∏è Empty response from fallback ${fallbackModelName}, marking as unreliable`);
              this.markModelAsUnreliable(fallbackModelName);
            } else {
              console.log(`‚úÖ Fallback ${fallbackModelName} responded successfully`);
              return response;
            }
          } catch (error) {
            console.warn(`‚ö†Ô∏è Fallback provider failed:`, error);
            this.markModelAsUnreliable(fallbackModelName);
          }
        }
      }
    }

    // Fallback to mock response if no providers available
    console.warn('‚ö†Ô∏è No LLM providers available, using mock response');
    return {
      content: `‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ OpenAI API key ‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö

‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: "${prompt}"

‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏° OPENAI_API_KEY ‡πÉ‡∏ô environment variables ‡∏Ñ‡∏£‡∏±‡∏ö`,
      usage: {
        prompt_tokens: prompt.length / 4,
        completion_tokens: 50,
        total_tokens: prompt.length / 4 + 50
      },
      model: request.model || 'mock',
      responseTime: 100
    };
  }

  private getProvider(modelName: string): LLMProvider | undefined {
    // Simple mapping - can be enhanced
    if (modelName.includes('gpt') || modelName.includes('openai')) {
      return this.providers.get('openai');
    }
    return undefined;
  }

  /**
   * Mark a model as unreliable and blacklist it
   */
  private markModelAsUnreliable(modelName: string): void {
    this.unreliableModels.add(modelName);
    console.warn(`üö´ Model ${modelName} marked as unreliable and blacklisted`);
  }

  /**
   * Check if a model is blacklisted
   */
  private isModelReliable(modelName: string): boolean {
    return !this.unreliableModels.has(modelName);
  }

  /**
   * Reset model reliability tracking (for testing or recovery)
   */
  public resetModelReliability(): void {
    this.unreliableModels.clear();
    console.log('üîÑ Model reliability tracking reset');
  }

  getUsage(): Record<string, TokenUsage> {
    const usage: Record<string, TokenUsage> = {};
    for (const [name, provider] of this.providers) {
      usage[name] = provider.getUsage();
    }
    return usage;
  }

  async isReady(): Promise<boolean> {
    return this.config !== null && this.systemPrompts.size > 0;
  }

  /**
   * ‡πÑ‡∏î‡πâ model name ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
   */
  getCurrentModel(): string {
    return this.config?.name || 'gpt-4o-mini';
  }

  /**
   * ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ model ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á
   */
  getModelConstraints(): { requiresDefaultTemperature?: boolean } {
    const model = this.getCurrentModel();
    
    if (model.includes('gpt-5-nano')) {
      return { requiresDefaultTemperature: true };
    }
    
    return {};
  }
}