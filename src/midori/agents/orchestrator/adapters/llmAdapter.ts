/**
 * üéØ LLM Adapter - Main orchestrator LLM interface
 * ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ multiple providers ‡πÅ‡∏•‡∏∞ fallback logic
 * Server-side version (‡πÉ‡∏ä‡πâ fs/promises ‡πÑ‡∏î‡πâ)
 */

import fs from 'fs/promises';
import path from 'path';
import * as yaml from 'js-yaml';
import { OpenAIProvider } from './openaiProvider';
import { LLMConfig, LLMProvider, LLMRequest, LLMResponse, TokenUsage } from './types';

interface AgentConfig {
  model: LLMConfig;
}

export class LLMAdapter {
  private providers: Map<string, LLMProvider> = new Map();
  private config: LLMConfig | null = null;
  private systemPrompts: Map<string, string> = new Map();

  constructor() {
    // ‡πÑ‡∏°‡πà auto-initialize ‡πÉ‡∏ô constructor - ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å init methods ‡πÅ‡∏¢‡∏Å
  }

  async initialize(): Promise<void> {
    await this.initializeProviders();
    await this.loadConfig(); 
    await this.loadSystemPrompts();
  }

  private async initializeProviders(): Promise<void> {
    try {
      // Initialize OpenAI provider
      const apiKey = process.env.OPENAI_API_KEY;
      if (apiKey) {
        const openaiProvider = new OpenAIProvider(apiKey);
        this.providers.set('openai', openaiProvider);
        console.log('‚úÖ OpenAI provider initialized');
      } else {
        console.warn('‚ö†Ô∏è OpenAI API key not found');
      }
    } catch (error) {
      console.warn('‚ö†Ô∏è Failed to initialize LLM providers:', error);
    }
  }

  async loadConfig(): Promise<void> {
    try {
      // Use absolute path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ path ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ô‡∏à‡∏≤‡∏Å directory ‡∏≠‡∏∑‡πà‡∏ô
      const projectRoot = process.env.MIDORI_PROJECT_ROOT || process.cwd();
      console.log('üîç Loading config from:', { projectRoot, cwd: process.cwd() });
      const configPath = path.join(projectRoot, 'src/midori/agents/orchestrator/agent.yaml');
      console.log('üìÑ Config path:', configPath);
      const configFile = await fs.readFile(configPath, 'utf-8');
      const agentConfig = yaml.load(configFile) as AgentConfig;
      
      this.config = agentConfig.model;
      console.log('‚öôÔ∏è LLM config loaded:', {
        model: this.config.name,
        temperature: this.config.temperature,
        fallback: this.config.fallback?.name
      });
    } catch (error) {
      console.error('‚ùå Failed to load LLM config:', error);
      throw error;
    }
  }

  async loadSystemPrompts(): Promise<void> {
    try {
      const promptsPath = path.join(process.cwd(), 'src/midori/agents/orchestrator/prompts');
      
      const prompts = [
        { key: 'system', file: 'system-prompt.md' },
        { key: 'tasks', file: 'task_templates.md' },
        { key: 'guardrails', file: 'guardrails.md' }
      ];

      for (const { key, file } of prompts) {
        try {
          const content = await fs.readFile(path.join(promptsPath, file), 'utf-8');
          this.systemPrompts.set(key, content);
        } catch (error) {
          console.warn(`‚ö†Ô∏è Failed to load ${file}:`, error);
        }
      }

      console.log('‚úÖ System prompts loaded:', Array.from(this.systemPrompts.keys()));
    } catch (error) {
      console.error('‚ùå Failed to load system prompts:', error);
      throw error;
    }
  }

  async callLLM(
    prompt: string, 
    options: {
      useSystemPrompt?: boolean;
      model?: string;
      temperature?: number;
      maxTokens?: number;
      maxCompletionTokens?: number;
      reasoning?: {
        effort: 'minimal' | 'low' | 'medium' | 'high';
      };
      text?: {
        verbosity: 'low' | 'medium' | 'high';
      };
    } = {}
  ): Promise<LLMResponse> {
    if (!this.config) {
      throw new Error('LLM config not loaded. Call initialize() first.');
    }

    const config = this.config;

    // Build system prompt
    let systemPrompt = '';
    if (options.useSystemPrompt !== false) {
      const systemContent = this.systemPrompts.get('system') || '';
      const tasksContent = this.systemPrompts.get('tasks') || '';
      const guardrailsContent = this.systemPrompts.get('guardrails') || '';
      systemPrompt = `${systemContent}\n\n${tasksContent}\n\n${guardrailsContent}`;
    }

    const request: LLMRequest = {
      prompt,
      systemPrompt: systemPrompt || undefined,
      model: options.model || config.name,
      temperature: options.temperature ?? config.temperature,
      maxTokens: options.maxTokens || config.max_completion_tokens || config.max_tokens,
      maxCompletionTokens: options.maxCompletionTokens,
      reasoning: options.reasoning,
      text: options.text
    };

    // Try primary provider with retry logic
    const modelName = request.model || 'gpt-4o-mini';
    const primaryProvider = this.getProvider(modelName);
    
    if (primaryProvider && await primaryProvider.isAvailable()) {
      try {
        console.log(`üöÄ Calling ${modelName}...`);
        const response = await this.executeWithRetry(
          () => primaryProvider.call(request),
          {
            maxRetries: 3,
            baseDelay: 1000,
            maxDelay: 10000,
            retryableErrors: ['timeout', 'rate_limit', 'api_error', 'network_error']
          }
        );
        
        // Check for empty or invalid response
        if (!response?.content || response.content.trim() === '') {
          console.warn(`‚ö†Ô∏è Empty response from ${modelName}, trying fallback`);
        } else {
          console.log(`‚úÖ ${modelName} responded successfully`);
          return response;
        }
      } catch (error) {
        console.warn(`‚ö†Ô∏è Primary provider failed after retries:`, error);
        // Continue to fallback logic instead of re-throwing
      }
    }

    // Try fallback provider
    if (config.fallback) {
      const fallbackModelName = config.fallback.name;
      
      const fallbackProvider = this.getProvider(fallbackModelName);
      if (fallbackProvider && await fallbackProvider.isAvailable()) {
        try {
          console.log(`üîÑ Falling back to ${fallbackModelName}...`);
          const fallbackRequest = {
            ...request,
            model: fallbackModelName,
            temperature: config.fallback.temperature
          };
          
          const response = await this.executeWithRetry(
            () => fallbackProvider.call(fallbackRequest),
            {
              maxRetries: 2,
              baseDelay: 2000,
              maxDelay: 8000,
              retryableErrors: ['timeout', 'rate_limit', 'api_error', 'network_error']
            }
          );
          
          // Check fallback response quality
          if (!response?.content || response.content.trim() === '') {
            console.warn(`‚ö†Ô∏è Empty response from fallback ${fallbackModelName}, trying alternative`);
          } else {
            console.log(`‚úÖ Fallback ${fallbackModelName} responded successfully`);
            return response;
          }
        } catch (error) {
          console.warn(`‚ö†Ô∏è Fallback provider failed after retries:`, error);
        }
      }
      
      // Try alternative model as last resort
      if (fallbackModelName !== 'gpt-4o-mini') {
        console.log(`üîÑ Trying alternative model: gpt-4o-mini`);
        const alternativeProvider = this.getProvider('gpt-4o-mini');
        if (alternativeProvider && await alternativeProvider.isAvailable()) {
          try {
            const alternativeRequest = {
              ...request,
              model: 'gpt-4o-mini',
              temperature: 0.3
            };
            
            const response = await this.executeWithRetry(
              () => alternativeProvider.call(alternativeRequest),
              {
                maxRetries: 1,
                baseDelay: 3000,
                maxDelay: 5000,
                retryableErrors: ['timeout', 'rate_limit', 'api_error', 'network_error']
              }
            );
            
            if (response?.content && response.content.trim() !== '') {
              console.log(`‚úÖ Alternative gpt-4o-mini responded successfully`);
              return response;
            }
          } catch (error) {
            console.warn(`‚ö†Ô∏è Alternative provider failed:`, error);
          }
        }
      }
    }

    // Fallback to mock response if no providers available
    console.warn('‚ö†Ô∏è No LLM providers available, using mock response');
    return {
      content: `‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ OpenAI API key ‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö

‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: "${prompt}"

‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏° OPENAI_API_KEY ‡πÉ‡∏ô environment variables ‡∏Ñ‡∏£‡∏±‡∏ö`,
      usage: {
        prompt_tokens: prompt.length / 4,
        completion_tokens: 50,
        total_tokens: prompt.length / 4 + 50
      },
      model: request.model || 'mock',
      responseTime: 100
    };
  }

  private getProvider(modelName: string): LLMProvider | undefined {
    // Simple mapping - can be enhanced
    if (modelName.includes('gpt') || modelName.includes('openai')) {
      return this.providers.get('openai');
    }
    return undefined;
  }

  /**
   * Execute operation with retry logic
   */
  private async executeWithRetry<T>(
    operation: () => Promise<T>,
    options: {
      maxRetries: number;
      baseDelay: number;
      maxDelay: number;
      retryableErrors: string[];
    }
  ): Promise<T> {
    let lastError: any;
    
    for (let attempt = 0; attempt <= options.maxRetries; attempt++) {
      try {
        return await operation();
      } catch (error) {
        lastError = error;
        
        // Check if error is retryable
        if (!this.isRetryableError(error, options.retryableErrors)) {
          throw error;
        }
        
        if (attempt === options.maxRetries) {
          throw error;
        }
        
        // Calculate delay with exponential backoff and jitter
        const delay = Math.min(
          options.baseDelay * Math.pow(2, attempt),
          options.maxDelay
        );
        const jitter = Math.random() * 0.1 * delay;
        const finalDelay = delay + jitter;
        
        console.log(`üîÑ Retry ${attempt + 1}/${options.maxRetries} in ${Math.round(finalDelay)}ms`);
        await new Promise(resolve => setTimeout(resolve, finalDelay));
      }
    }
    
    throw lastError;
  }
  
  /**
   * Check if error is retryable
   */
  private isRetryableError(error: any, retryableErrors: string[]): boolean {
    const errorMessage = error.message?.toLowerCase() || '';
    return retryableErrors.some(retryableError => 
      errorMessage.includes(retryableError.toLowerCase())
    );
  }

  getUsage(): Record<string, TokenUsage> {
    const usage: Record<string, TokenUsage> = {};
    for (const [name, provider] of this.providers) {
      usage[name] = provider.getUsage();
    }
    return usage;
  }

  async isReady(): Promise<boolean> {
    return this.config !== null && this.systemPrompts.size > 0;
  }

  /**
   * ‡πÑ‡∏î‡πâ model name ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
   */
  getCurrentModel(): string {
    return this.config?.name || 'gpt-4o-mini';
  }

  /**
   * ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ model ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á
   */
  getModelConstraints(): { requiresDefaultTemperature?: boolean } {
    const model = this.getCurrentModel();
    
    if (model.includes('gpt-5-nano')) {
      return { requiresDefaultTemperature: true };
    }
    
    return {};
  }
}