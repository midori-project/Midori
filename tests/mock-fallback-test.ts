/**
 * ğŸ§ª Mock GPT-5-nano Fallback Test
 * à¸ˆà¸³à¸¥à¸­à¸‡à¸à¸¤à¸•à¸´à¸à¸£à¸£à¸¡ GPT-5-nano à¸—à¸µà¹ˆ return empty responses
 */

import { LLMAdapter } from '../src/midori/agents/orchestrator/adapters/llmAdapter';
import { LLMProvider, LLMRequest, LLMResponse, TokenUsage } from '../src/midori/agents/orchestrator/adapters/types';

// Mock Provider that simulates GPT-5-nano empty responses
class MockGPT5NanoProvider implements LLMProvider {
  public name = 'mock-openai';
  private callCount = 0;
  private usage: TokenUsage = { promptTokens: 0, completionTokens: 0, totalTokens: 0 };

  async isAvailable(): Promise<boolean> {
    return true;
  }

  async call(request: LLMRequest): Promise<LLMResponse> {
    this.callCount++;
    
    if (request.model === 'gpt-5-nano') {
      // Simulate GPT-5-nano consistently returning empty responses
      console.log(`ğŸ”¥ MockGPT5NanoProvider: Call #${this.callCount} to ${request.model} - returning empty response`);
      this.usage.promptTokens += request.prompt.length / 4;
      return {
        content: '', // Empty response!
        usage: { 
          prompt_tokens: request.prompt.length / 4,
          completion_tokens: 0,
          total_tokens: request.prompt.length / 4 
        },
        model: request.model,
        responseTime: 1200
      };
    }
    
    if (request.model === 'gpt-4o-mini') {
      // Simulate successful fallback
      console.log(`âœ… MockGPT5NanoProvider: Call #${this.callCount} to ${request.model} - returning proper response`);
      const response = `à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š! à¸œà¸¡à¹€à¸›à¹‡à¸™ AI Assistant à¸—à¸µà¹ˆà¸à¸£à¹‰à¸­à¸¡à¸Šà¹ˆà¸§à¸¢à¹€à¸«à¸¥à¸·à¸­à¸„à¸¸à¸“

à¸ªà¸³à¸«à¸£à¸±à¸šà¸„à¸³à¸ªà¸±à¹ˆà¸‡: "${request.prompt}"

à¸œà¸¡à¸ªà¸²à¸¡à¸²à¸£à¸–à¸Šà¹ˆà¸§à¸¢à¸„à¸¸à¸“à¹„à¸”à¹‰à¹ƒà¸™à¸«à¸¥à¸²à¸¢à¹€à¸£à¸·à¹ˆà¸­à¸‡ à¹€à¸Šà¹ˆà¸™:
- à¸à¸²à¸£à¹€à¸‚à¸µà¸¢à¸™à¹‚à¸„à¹‰à¸” à¹à¸¥à¸° debugging
- à¸à¸²à¸£à¸­à¸­à¸à¹à¸šà¸šà¸£à¸°à¸šà¸š
- à¸à¸²à¸£à¹à¸à¹‰à¹„à¸‚à¸›à¸±à¸à¸«à¸²
- à¸à¸²à¸£à¸­à¸˜à¸´à¸šà¸²à¸¢à¹à¸™à¸§à¸„à¸´à¸”à¸—à¸²à¸‡à¹€à¸—à¸„à¸™à¸´à¸„

à¸¡à¸µà¸­à¸°à¹„à¸£à¹ƒà¸«à¹‰à¸Šà¹ˆà¸§à¸¢à¹€à¸«à¸¥à¸·à¸­à¹„à¸«à¸¡à¸„à¸£à¸±à¸š?`;

      this.usage.promptTokens += request.prompt.length / 4;
      this.usage.completionTokens += response.length / 4;
      this.usage.totalTokens = this.usage.promptTokens + this.usage.completionTokens;
      
      return {
        content: response,
        usage: { 
          prompt_tokens: request.prompt.length / 4,
          completion_tokens: response.length / 4,
          total_tokens: request.prompt.length / 4 + response.length / 4
        },
        model: request.model,
        responseTime: 800
      };
    }
    
    throw new Error(`Unsupported model: ${request.model}`);
  }

  getUsage(): TokenUsage {
    return this.usage;
  }
}

async function testMockFallback() {
  console.log('ğŸ§ª Testing Mock GPT-5-nano Fallback Behavior...\n');

  // Create adapter and inject mock provider
  const adapter = new LLMAdapter();
  
  // Inject mock provider
  const mockProvider = new MockGPT5NanoProvider();
  (adapter as any).providers.set('openai', mockProvider);
  
  // Load config and prompts
  await adapter.loadConfig();
  await adapter.loadSystemPrompts();

  console.log('ğŸ“ Test 1: GPT-5-nano Empty Response -> Should fallback to gpt-4o-mini');
  try {
    const response1 = await adapter.callLLM('à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š à¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸šà¸š', {
      model: 'gpt-5-nano',
      temperature: 1
    });
    
    console.log('ğŸ“Š Final Model Used:', response1.model);
    console.log('ğŸ“ Response Content:', response1.content ? response1.content.substring(0, 150) + '...' : 'EMPTY');
    console.log('â±ï¸ Response Time:', response1.responseTime, 'ms');
    console.log('ğŸ›¡ï¸ Unreliable Models:', Array.from((adapter as any).unreliableModels));
    console.log('');
  } catch (error) {
    console.error('âŒ Test 1 failed:', error);
  }

  console.log('ğŸ“ Test 2: Second request -> Should skip gpt-5-nano (blacklisted) and go directly to fallback');
  try {
    const response2 = await adapter.callLLM('à¸ªà¸£à¹‰à¸²à¸‡à¸«à¸™à¹‰à¸²à¹€à¸§à¹‡à¸š landing page', {
      model: 'gpt-5-nano',
      temperature: 1
    });
    
    console.log('ğŸ“Š Final Model Used:', response2.model);
    console.log('ğŸ“ Response Content:', response2.content ? response2.content.substring(0, 150) + '...' : 'EMPTY');
    console.log('â±ï¸ Response Time:', response2.responseTime, 'ms');
    console.log('ğŸ›¡ï¸ Unreliable Models:', Array.from((adapter as any).unreliableModels));
    console.log('');
  } catch (error) {
    console.error('âŒ Test 2 failed:', error);
  }

  console.log('ğŸ“ Test 3: Reset reliability and try again');
  adapter.resetModelReliability();
  try {
    const response3 = await adapter.callLLM('à¸—à¸”à¸ªà¸­à¸šà¸«à¸¥à¸±à¸‡ reset', {
      model: 'gpt-5-nano',
      temperature: 1
    });
    
    console.log('ğŸ“Š Final Model Used:', response3.model);
    console.log('ğŸ“ Response Content:', response3.content ? response3.content.substring(0, 150) + '...' : 'EMPTY');
    console.log('â±ï¸ Response Time:', response3.responseTime, 'ms');
    console.log('ğŸ›¡ï¸ Unreliable Models:', Array.from((adapter as any).unreliableModels));
    console.log('');
  } catch (error) {
    console.error('âŒ Test 3 failed:', error);
  }

  console.log('ğŸ Mock fallback test completed!');
  console.log('ğŸ“Š Total Provider Usage:', mockProvider.getUsage());
}

// Run the test
if (require.main === module) {
  testMockFallback().catch(console.error);
}

export { testMockFallback };